[
["index.html", "STAT 394: Causal Inference Welcome!", " STAT 394: Causal Inference Welcome! Image source: IMDB. (Know how this relates to class? Tell the instructor!) This is the course website for STAT 394: Causal Inference at Macalester College for the Spring 2020 semester. The content here was made by Leslie Myint and draws upon several resources, all of which are listed on the References page. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["references.html", "References", " References The material for this course draws from the following list of excellent resources. Required references (download these!): Causal Inference in Statistics: A Primer, by Judea Pearl, Madelyn Glymour, and Nicholas P. Jewell. PDF freely available online. (Referred to as “PRIMER”.) Causal Inference: What If, by Miguel A. Hernán and James M. Robins. PDF freely available online. (Referred to as “WHATIF”.) Other materials: The Book of Why: The New Science of Cause and Effect, by Judea Pearl and Dana Mackenzie (Amazon) Causality: Models, Reasoning, and Inference, by Judea Pearl (available as an eBook through Macalester’s libarary) Materials from the course, Causal Inference in Medicine and Public Health, taught by Elizabeth Stuart at JHSPH edX course: Causal Diagrams: Draw Your Assumptions Before Your Conclusions Udemy course: Causal Data Science with Directed Acyclic Graphs "],
["schedule.html", "Schedule", " Schedule Week 1 Thursday, January 23 Introductions, review of key ideas from STAT 155, why we need causal stories (graphs!) Slides from class are available here. After class: Recommended reading: PRIMER, Chapter 1: Sections 1.1, 1.2 Required reading: PRIMER, Chapter 1: 1.4, 1.5.1 Week 2 Thursday, January 30 The mathematics (probability) of graphs Before class: Please watch the following two videos and/or read the corresponding sections of PRIMER. Answer the Moodle questions before class. Video 1: Essentials of Probability, (slides) Video 2: Key Structure in DAGs, (slides) Reading: PRIMER, Chapter 1: Sections 1.3.1 to 1.3.5, Chapter 2: Sections 2.1 to 2.3 Week 3 Tuesday, February 4 d-separation and its applications for understanding bias Before class: Reading: PRIMER, Chapter 2: Section 2.4. There is one associated Moodle question for this reading. Please also read the article The Birth Weight “Paradox” Uncovered? available in the Readings section on Moodle. We will discuss in class on Tuesday. Tuesday, February 4 d-separation and its applications for understanding bias (continued) No new reading. Work on Homework 1, due Thursday, February 13 in class. Week 4 Tuesday, February 11 The do-operator and estimating causal effects Before class: Required reading: PRIMER, Chapter 1: 1.3.6 and 1.3.7, Chapter 3: Sections 3.1 through 3.3 (skip 3.2.2 on Multiple Interventions). Answer the Moodle questions (under “The do-operator and estimating causal effects”) before class. Thursday, February 13 The do-operator and estimating causal effects (continued) Week 5 Tuesday, February 18 The do-operator and estimating causal effects (continued) Thursday, February 20 Inverse probability weighting and structural models Before class: Required reading: WHATIF Sections 1.1 to 1.3 (Chapter 1: A definition of causal effect) and Chapter 12: IP weighting and marginal structural models. No Moodle questions this week. Just read to familiarize yourselves with the ideas, which we’ll discuss in class this week. Chapter 12 is a little more technical - let the following questions guide your reading: What is the concept of the “pseudo-population”? What is the nice characteristic that it has? How does IP weighting achieve that characteristic? How are stabilized IP weights different from ordinary (unstabilized) weights? Why would we want to use stabilized weights? What is a marginal structural model, and how are they fit? What parameters of these models are of interest? How is IP weighting used in dealing with censoring (loss-to-follow up / selection bias)? Week 6 Tuesday, February 25 IP weighting to account for selection bias Thursday, February 27 No class - Capstone Days! Week 7 Tuesday, March 3 Causal discovery Thursday, March 5 Causal discovery Week 8 Tuesday, March 3 Sensitivity analyses for unmeasured confounding Thursday, March 5 Project work day Spring Break: 3/16 - 3/27 Week 9: 3/30 - 4/3 If Google/YouTube access is a problem, all videos and slides are also available on Moodle in the “Videos &amp; Slides” section. All Moodle questions are in the “Reading/Video Questions” section. Tuesday, March 31 Mediation analysis (video, slides) Moodle questions: Mediation Analysis due Wednesday, April 1 at midnight (CST) Thursday, April 2 Mediation analysis (continued) Week 10: 4/6 - 4/10 Tuesday, March 31 Randomized Controlled Trials (video, slides) Quasi-Experimental Study Designs (video, slides) Moodle questions: Study Designs due Wednesday, April 8 at midnight (CST) Thursday, April 2 Study designs (continued) "],
["introductions-review-and-motivation.html", "Topic 1 Introductions, Review, and Motivation Learning Goals Review: Regression Models Exercises", " Topic 1 Introductions, Review, and Motivation Slides from today are available here. Learning Goals Understand the instances in which we care about causation more than association Review regression modeling and develop some ideas about its relation to causal inference goals Develop some ideas for why causal stories (i.e., expert knowledge) are crucial for causal inference goals Review: Regression Models Discuss the following questions with your group members. What does a linear regression model formula look like? If you covered logistic regression in STAT 155, what does a logistic regression model formula look like? When would you use logistic vs. linear regression? In general, how do we interpret the intercept in a regression model (both linear and logistic)? How do we interpret the other coefficients in a regression model (both linear and logistic)? How can we justify this interpretation mathematically? How might regression models be useful in understanding the causal effect of some variable on an outcome? Exercises You can download a template RMarkdown file to start from here. We will look at (simulated) data from a study that looked at the effectiveness of chemotherapy for treating colon cancer. Chemotherapy is effectively a poison that kills cells in the body that are rapidly proliferating: these cells include the cancer cells (often in a mass called a tumor) but also cells in bone marrow involved in sustaining the immune system. In this study, researchers measured the following variables: pre_tumor_size: Tumor size before the chemotherapy/placebo treatment (Small or Large). treated: ChemoYes if the patient received chemotherapy or ChemoNo if not. post_tumor_size: Tumor size 3 months after the chemotherapy/placebo treatment (Small or Large). recovery: Yes if the patient recovered from their cancer. No otherwise. You can read in the data as follows: chemo_study_data &lt;- read.csv(&quot;https://www.dropbox.com/s/vl06j75a8afw8ct/chemo_study.csv?dl=1&quot;) Exercise 1 The first step in any data analysis is to visualize your data. Let’s refamiliarize ourselves with the ggplot2 package in R. It may be helpful to have this ggplot2 cheat sheet open. Make sure to load the ggplot2 package by including library(ggplot2) at the top of your RMarkdown document. Look at the distribution of each of the 4 measured variables. What plot type is most appropriate for this type of variable? ggplot(chemo_study_data, aes(x = pre_tumor_size)) + ??? Is pre-treatment tumor size predictive of whether or not a patient received chemotherapy? Make a plot to assess this, and briefly state what conclusions can be drawn from the plot. (Hint: it will be helpful to look at the second page of the cheat sheet in the section labeled “Position Adjustments”.) ggplot(chemo_study_data, aes(x = pre_tumor_size, fill = treated)) + ??? Is pre-treatment tumor size predictive of whether or not a patient recovered? Make a plot, and briefly state your conclusions. A variable is a confounder if it is a common cause of both the treatment and outcome. This is shown in the diagram below. Given your results from parts b and c, could pre-treatment tumor size be a confounder of the relationship between chemotherapy treatment and recovery? If yes, what is the concern here? Note: the causal relationships of interest so far can be depicted in a causal diagram, shown below. An arrow between two variables indicates that one is a cause of the other (an arrow points from a cause to its effect). Exercise 2 One way to “adjust for” the influence of counfounders is to include them as predictors/explanatory variables in regression models. We can model recovery using a logistic regression model (used when the outcome variable is binary). In R, we can fit a logistic regression model using code like the following: # Fit the model and store it in the &quot;mod&quot; object mod &lt;- glm(outcome_variable ~ explanatory_variable1+explanatory_variable2, family = &quot;binomial&quot;, data = your_data) # Display model output summary(mod) Fit a logistic regression model with only treatment as a predictor. Interpret the treatment coefficient. Is this the interpretation you expected? Fit a logistic regression model with both treatment and pre-treatment tumor size as predictors. Interpret the treatment coefficient. Is this the interpretation you expected? Exercise 3 So far, we have not considered the post_tumor_size variable at all in our analyses. Let’s get some visual understanding of this variable. Make 2 plots: one showing its relationship with the treated variable and the second showing its relationship with the recovery variable. What is your intuition - should we include post_tumor_size as an explanatory variable in our logistic regression model? Why or why not? Fit the logistic regression model (mod3) with all 3 explanatory variables, and interpret the treatment coefficient. How do the results compare to mod2? The diagram that we considered at the end of Exercise 1 only included the treatment, recovery, and pre-treatment tumor size variables. Draw (on paper) an expanded causal diagram that includes post-treatment tumor size. In light of where post-treatment tumor size appears on your diagram, what does it mean to include it in the logistic regression model? The data in this investigation were simulated. That is, your instructor generated them, and thus, she actually knows the truth behind the data. The truth is that chemotherapy does have a true beneficial causal impact on recovery. Chemotherapy does increases the chance of recovery. Given this information and the insights you gained from these three exercises, try to put all of this information together. What makes sense? What remains unclear? "],
["probability-and-dags.html", "Topic 2 Probability and DAGs Learning Goals Goal 1 Lead-in to R activity: Poll Discussion R Exercises Conceptual Exercises", " Topic 2 Probability and DAGs Learning Goals Appreciate the importance of starting analysis from prior/expert knowledge Simulate data corresponding to a causal DAG Use simulations to verify the marginal/conditional independence and dependence relationships implied by DAG structures Develop notions of causal and non-causal associations in the context of graphs Goal 1 In your groups, discuss your responses to the pre-class question about the figure below (which came from this article from Nature Pediatric Research). What arrows do all of you agree on? Do you disagree on any arrows? If the question of interest is how screen time affects the risk of childhood obesity, what variables and arrows do you think are missing? If you have time left, pick your favorite example of conditional independence and conditional dependence from your answers to the pre-class questions. Lead-in to R activity: Poll Navigate to: PollEv.com/lesliemyint417 Discussion Download this Rmd file to start from. library(ggplot2) library(dplyr) An introduction to the rbinom function: # 4 different people each flip a fair coin once rbinom(4, size = 1, prob = 0.5) # 4 different people flip loaded coins # First 2 flip a coin with P(Heads) = 0.9 # Second 2 flip a coin with P(Heads) = 0.2 rbinom(4, size = 1, prob = c(0.9, 0.9, 0.2, 0.2)) # Before you run this code, what will the result be? rbinom(2, size = 1, prob = c(1, 0)) How can we simulate X -&gt; Y for binary X and Y? # set.seed() ensures reproducible random numbers set.seed(2020) # Set sample size n &lt;- 1e5 # Generate a binary variable X for all n cases X &lt;- rbinom(n, size = 1, prob = 0.5) Y &lt;- rbinom(n, size = 1, prob = 0.5) Are the X and Y we just simulated independent or dependent? Let’s make a plot to see: # We put X and Y in a dataset using data.frame() # They are turned into categorical variables using factor() sim_data &lt;- data.frame(X = factor(X), Y = factor(Y)) # Proportional bar plot ggplot(sim_data, aes(x = X, fill = Y)) + geom_bar(position = &quot;fill&quot;) How can we make Y dependent on X? # Generate probabilities that Y=1 that depend on the value of X p_Y &lt;- dplyr::case_when( X==1 ~ 0.8, X==0 ~ 0.3 ) # Simulate Y using p_Y Y &lt;- rbinom(n, size = 1, prob = p_Y) # Put X, Y, and p_Y into a dataset sim_data &lt;- data.frame(X = factor(X), Y = factor(Y), p_Y = p_Y) # Check that p_Y = 0.8 when X is 1 sim_data %&gt;% filter(X==&quot;1&quot;) # Check that p_Y = 0.3 when X is 0 sim_data %&gt;% filter(X==&quot;0&quot;) # Proportional bar plot ggplot(sim_data, aes(x = X, fill = Y)) + geom_bar(position = &quot;fill&quot;) How can we make a variable Z dependent on 2 causes: X and Y? X &lt;- rbinom(n, size = 1, prob = 0.5) Y &lt;- rbinom(n, size = 1, prob = 0.5) p_Z &lt;- dplyr::case_when( X==1 &amp; Y==1 ~ 0.8, X==0 &amp; Y==1 ~ 0.3, X==1 &amp; Y==0 ~ 0.5, X==0 &amp; Y==0 ~ 0.9 ) Z &lt;- rbinom(n, size = 1, prob = p_Z) # Put X, Y, and Z into a dataset sim_data &lt;- data.frame(X = factor(X), Y = factor(Y), Z = factor(Z)) # If we wanted to save datasets filtered on Z (Why?) sim_data_filt_z0 &lt;- sim_data %&gt;% filter(Z==&quot;0&quot;) sim_data_filt_z1 &lt;- sim_data %&gt;% filter(Z==&quot;1&quot;) R Exercises Exercise 1 Simulate data for a chain X -&gt; Y -&gt; Z where all variables are binary. Make plots to show the following properties: X and Z are marginally dependent X and Z are conditionally independent given Y Describe how your simulation would change if you wanted to check the more general property of chains that two variables are conditionally independent given any variables in between them. (You don’t actually need to implement this simulation but can if you have time.) Exercise 2 Simulate data for a fork Y &lt;- X -&gt; Z where all variables are binary. Make plots to show the following properties: Y and Z are marginally dependent Y and Z are conditionally independent given X Exercise 3 Simulate data for a collider X -&gt; Z &lt;- Y where all variables are binary. Make plots to show the following properties: X and Y are marginally independent X and Y are conditionally dependent given Z Describe how your simulation would change if you wanted to check the more general property of colliders that X and Y become dependent conditional on Z and/or any descendants of Z. (You don’t actually need to implement this simulation but can if you have time.) Conceptual Exercises Exercise 4 In the causal diagram above, let X be a treament and Y be an outcome of interest. What would you say are the “causal paths” between X and Y? What would you say are the “non-causal paths”? Thinking about the marginal and conditional independence and dependence relations we’ve discussed, how might we restrict the influence of these non-causal paths? Exercise 5 Can you come up with a general rule that tells us when two variables will be conditionally independent given another set of variables Z? (Where it is possible that Z is an empty set containing no variables.) Try out your rule on the causal diagram below. "],
["d-separation-part-1.html", "Topic 3 d-separation (Part 1) Learning Goals Discussion: d-separation Discussion: The Birth Weight Paradox", " Topic 3 d-separation (Part 1) Learning Goals Review concepts of simulating DAGs in R Understand d-separation as a general means of identifying independence/dependence in arbitrarily complex causal diagrams Apply d-separation ideas to a case study: the birth weight paradox Discussion: d-separation A path \\(p\\) is blocked by a set of nodes \\(Z\\) (which could be the empty set) if and only if at least one of the two conditions below are met. (\\(Z\\) is the set of variables being conditioned on.) \\(p\\) contains a chain of nodes \\(A \\rightarrow B \\rightarrow C\\) or a fork \\(A \\leftarrow B \\rightarrow C\\) such that the middle node \\(B\\) is in \\(Z\\). \\(p\\) contains a collider \\(A \\rightarrow B \\leftarrow C\\) such that the collision node B is not in \\(Z\\), and no descendant of \\(B\\) is in \\(Z\\). If \\(Z\\) blocks every path between two nodes \\(X\\) and \\(Y\\), then \\(X\\) and \\(Y\\) are d-separated, conditional on \\(Z\\), and are thus independent conditional on \\(Z\\). Discussion: The Birth Weight Paradox To facilitate our discussion, key pieces of information and terminology from the article are summarized below. Crude mortality rate ratio \\[\\frac{\\hbox{Infant mortality rate for maternal smokers}}{\\hbox{Infant mortality rate for maternal non-smokers}} = 1.55\\] Adjusted mortality rate ratio: same ratio but arising from a logistic regression model in which birth weight was held constant. This was 1.09. Stratum-specific mortality rate ratios In low birth weight infants: \\[\\frac{\\hbox{Infant mortality rate for maternal smokers}}{\\hbox{Infant mortality rate for maternal non-smokers}} = 0.79\\] In normal birth weight infants: \\[\\frac{\\hbox{Infant mortality rate for maternal smokers}}{\\hbox{Infant mortality rate for maternal non-smokers}} = 1.80\\] In your groups, discuss the following questions. Make sure to take good notes because the first post in your Portfolio will be a careful analysis of this paper. Throughout, remember that looking only at low birth weight infants amounts to conditioning on that variable. Using our knowledge of (in)dependence relations in DAGs, explain this quote about Figure 3.1 from the paper: “Under this scenario, the crude mortality rate ratio for smoking would be greater than 1, whereas the adjusted rate ratio and, equivalently, the stratum-specific rate ratios should be 1.” Explain why Figure 3.2 implies that the crude and adjusted mortality rate ratios would be the same. In Figure 3.5, the authors do not include an arrow from Smoking to Mortality, but they use this diagram to explain how the birth weight paradox could arise. Why is excluding this arrow a key part of their argumentation? Using Figure 3.5, explain how the paradox arises by using the ideas of d-separation. Figure 3.7 is the most realistic of the causal diagrams. Explain how the paradox arises in this setting. In Figure 3.7, would the paradox have arisen if LBW-Type A had been the selection criterion instead of LBW? Would the paradox have arisen if LBW-Type B had been the selection criterion? Let’s generalize the key findings in this paper. The term selection bias broadly describes bias in results due to the way in which study participants are selected. How did selection bias come about for the birth weight paradox? Can we generalize this occurrence in terms of more general graph properties? "],
["d-separation-part-2.html", "Topic 4 d-separation (Part 2) Learning Goals Why care? Example: Folic Acid and Birth Defects Example: Selection Bias Example: Estrogens and Uterine Cancer Principles of building causal diagrams", " Topic 4 d-separation (Part 2) Learning Goals Practice d-separation ideas with more examples Why care? In adjusting for variables in our analysis, we want to “do no harm”: Block non-causal paths that generate unwanted associations Do not accidentally create non-causal paths that generate unwanted associations Leave causal paths (chains) alone Example: Folic Acid and Birth Defects Does maternal folic acid supplementation reduce the risk of birth defects? Or could associations be due to confounding factors? What should we adjust for in our analysis? Example: Selection Bias See handout! Example: Estrogens and Uterine Cancer Does postmenopausal estrogen supplementation (hormone replacement therapy) cause uterine cancer? Consistent association between estrogen use and uterine cancer was noticed in the 1970s Two hypotheses: Estrogens do cause cancer Estrogens don’t cause cancer but lead to uterine bleeding, leading to more frequent doctor visits, leading to increased diagnosis of existing cancer Proposal for a study: restrict the study only to those with uterine bleeding and compare cancer rates in estrogen-users and non-users In this way, all participants have the same chance of being diagnosed. What could be wrong about this approach? Can we design a better study? Causal diagrams corresponding to the two hypotheses: Work through the following questions in your groups: Consider the study proposal: restrict analysis to those with uterine bleeding. Argue that under DAG 1, estrogens and diagnosed cancer will be associated. Argue that under DAG 2, estrogens and diagnosed cancer will be associated. Thus conclude that this study proposal cannot distinguish between the two competing hypotheses. Consider another study proposal: ensure that everyone is screened frequently, and we don’t restrict our analysis to only those with uterine bleeding. What arrow (in either DAG 1 or 2) can be removed as a result of this study design? In this study, say that we don’t find an association between estrogens and diagnosed cancer? What does this mean about paths from estrogens to diagnosed cancer? In this study, say that we do find an association between estrogens and diagnosed cancer? What does this mean about paths from estrogens to diagnosed cancer? Based on these investigations, make a conclusion about the quality of this study proposal as compared to the first. Principles of building causal diagrams A DAG is a causal DAG if it is common cause-complete: for any two variables in the DAG, common causes (whether measured or unmeasured) of those variables are shown. A causal DAG does NOT need to be cause-complete (infeasible due to infinite regress of causes). It should contain variables that are selected on, and subsequently common causes between those variables and existing variables. "],
["estimating-causal-effects.html", "Topic 5 Estimating causal effects Learning Goals Exercise: DAG construction Warm-up: d-separation Discussion: Estimating causal effects", " Topic 5 Estimating causal effects Learning Goals Practice constructing a causal diagram from the ground up in consultation with fellow experts Extend d-separation ideas to the estimation of causal effects Develop the idea of inverse probability of treatment weighting (IPW or IPTW) for causal effect estimation Understand how the do-operator is related to inverse probability weighting Exercise: DAG construction Research question: What is the causal effect of participating in yoga once per week for 12 weeks on resting heart rate at the end of that period? We are trying to design an observational study that could be carried out at Macalester. Recall our principles for constructing causal DAGs: A DAG is a causal DAG if it is common cause-complete: for any two variables in the DAG, common causes (whether measured or unmeasured) of those variables are shown. A causal DAG does NOT need to be cause-complete (infeasible due to infinite regress of causes). It should contain variables that are selected on, and subsequently common causes between those variables and existing variables. Reflect: Write about the process of constructing the causal diagram with your colleagues. How did your discussions flow? How did you resolve (or not) any disagreements on the structure? Were you able to decide if your diagram was good enough? If so, how? What questions or concerns do you still have about this process? Further reading: Chapter 9 of our WHATIF book (see References) introduces the consideration of measurement error. We won’t talk about measurement error in this course but you are welcome to read about these ideas on your own. Chapters 6, 7, and 8 talk about causal diagrams, confounding, and selection bias. This another resource to complement PRIMER. Warm-up: d-separation Navigate to: PollEv.com/lesliemyint417 Slides of the causal diagrams (with solutions) are available here. Discussion: Estimating causal effects In adjusting for variables in our analysis, we want to “do no harm”: Block non-causal paths that generate unwanted associations Do not accidentally create non-causal paths that generate unwanted associations Leave causal paths (chains) alone This was actually the rationale behind the backdoor criterion. (We’ll get back to this soon.) The do-operator One notion of causation is that of intervention: setting a variable’s value There is a difference between \\(P(Y \\mid A = a)\\) and \\(P(Y \\mid \\hbox{do}(A = a))\\). \\(P(Y \\mid A = a)\\) is an observational probability \\(P(Y \\mid \\hbox{do}(A = a))\\) is an intervention probability When we intervene on \\(A\\), this amounts to removing all arrows into \\(A\\). The do-operator allows us to graphically simulate interventions. In the world that was intervened on (manipulated), what do relationships between variables look like? Rules of probability combined with a set of graph rules (called the do-calculus) allow us to relate the relationships in the manipulated graph to relationships that we observe in the real world. \\[ \\hbox{Average causal effect} = P(Y = y \\mid \\hbox{do}(A = 1)) - P(Y = y \\mid \\hbox{do}(A = 0)) \\] \\(Y\\) is the outcome and \\(A\\) is the treatment (action) variable Taste of do-calculus next week. For now, let’s approach effect estimation from a different angle: The mathematical expressions in PRIMER for \\(P(y \\mid \\hbox{do}(a))\\) had something in common –&gt; let’s develop this idea and relate to something familiar: d-separation Worksheet: developing ideas Worksheet (with solutions) available here Notes: deriving inverse probability weighting If treatment \\(A\\) and outcome \\(Y\\) are d-separated given \\(Z\\) under the null (under the null hypothesis of no causal effect), then there are no open “spurious” paths generating unwanted associations between \\(A\\) and \\(Y\\). No open backdoor paths (paths with arrows into \\(A\\)): these generate confounding. No paths with collision nodes or descendants of collision nodes that are conditioned on: this opens paths that would otherwise be naturally blocked by the collision nodes. Then the outcomes of the treated and untreated are “comparable”. No open spurious paths. If the outcomes of the treated and untreated are “comparable”, then… We can upweight those treated to “create” a population where everyone was treated –&gt; look at outcome distributions in this population (e.g., \\(P(Y = 1)\\)). These weights are the inverse of \\(P(A = 1\\mid Z)\\). We can upweight those not treated to “create” a population where everyone was not treated –&gt; look at outcome distributions in this population (e.g., \\(P(Y = 1)\\)). These weights are the inverse of \\(P(A = 0\\mid Z)\\). These outcome distributions are actually interpreted as: All treated: \\(P(Y = 1 \\mid \\hbox{do}(A = 1))\\) All not treated: \\(P(Y = 1 \\mid \\hbox{do}(A = 0))\\) Note: we have appealed to what it conceptually means to do/intervene. We haven’t done any “graph surgery”. \\[ \\hbox{Average causal effect} = \\hbox{ACE} = P(Y = 1 \\mid \\hbox{do}(A = 1)) - P(Y = 1 \\mid \\hbox{do}(A = 0)) \\] If ACE = 0.1, interpreted as: The probability of recovery (\\(Y = 1\\)) is 10% higher if we treat everyone, as compared to if we treat no one. The individual “do” probabilities are actually weighted averages of the outcome variable: \\[ P(Y = 1 \\mid \\hbox{do}(A = a)) = \\frac{\\sum_i w_i y_i}{\\sum_i w_i} \\] \\(w_i\\) is the appropriate weight for case \\(i\\) \\(y_i\\) is the outcome for case \\(i\\) The sum is over all treated for \\(a = 1\\) and over all untreated for \\(a = 0\\). Punchline: If treatment \\(A\\) and outcome \\(Y\\) are d-separated given \\(Z\\) under the null, we can estimate desired “do” probabilities with \\[ P(Y = 1 \\mid \\hbox{do}(A = a)) = \\frac{\\sum_i w_i y_i}{\\sum_i w_i} \\] where the weights \\(w_i\\) are the inverse propensity scores: \\(w_i = 1/P(A = 1 \\mid Z)\\) if \\(a = 1\\) \\(w_i = 1/P(A = 0 \\mid Z)\\) if \\(a = 0\\) When \\(Z\\) contains many variables, we can’t tabulate as we did by hand (too many combinations of predictor levels relative to our sample size). Need to model treatment as a function of the variables in \\(Z\\). Logistic regression Other techniques from Statistical Machine Learning Exercise: Simulation planning Goal: Plan (then implement) a simulation that shows that inverse probability of treatment weighting (IPTW) to estimate average causal effects gives the same results as using the do-operator. This will be done in the context of the DAG below. Plan: As you go through this planning, think about what code you would have to write (in broad terms) and the order in which you would have to run those commands. For this exercise, don’t look at the code below. We’ll get to it next. Phase 1: How would you simulate the DAG below? (All variables binary.) How would you simulate new versions of Y under the manipulated graphs resulting from do(A = 1) and do(A = 0)? How would you compute the average causal effect from do-ing? Phase 2: How would you estimate the propensity score for each individual? How would you estimate P(Y = 1 | do(A = 1)) and P(Y = 1 | do(A = 0)) using those estimated propensity scores? After your planning phase, step through the code in the section below, and make sure that you understand what each line is doing. Clarify with the instructor as needed. Simulation example A template Rmd is available here. Task 1: What is the point of this simulation exercise? What are we trying to show? Clearly explain how we are approaching causal effect estimation from two seemingly different viewpoints. Task 2: Explain in detail all steps taken in this code. Do this by breaking the code into smaller code chunks and adding text in between. library(dplyr) Phase 1: set.seed(22) n &lt;- 1e6 Z &lt;- rbinom(n, size = 1, prob = 0.5) p_A &lt;- dplyr::case_when( Z==1 ~ 0.8, Z==0 ~ 0.3 ) A &lt;- rbinom(n, size = 1, prob = p_A) p_Y &lt;- dplyr::case_when( Z==1 &amp; A==1 ~ 0.3, Z==1 &amp; A==0 ~ 0.6, Z==0 &amp; A==1 ~ 0.9, Z==0 &amp; A==0 ~ 0.2 ) Y &lt;- rbinom(n, size = 1, prob = p_Y) A_do_1 &lt;- rep(1, n) A_do_0 &lt;- rep(0, n) p_Y_Ado1 &lt;- dplyr::case_when( Z==1 &amp; A_do_1==1 ~ 0.3, Z==1 &amp; A_do_1==0 ~ 0.6, Z==0 &amp; A_do_1==1 ~ 0.9, Z==0 &amp; A_do_1==0 ~ 0.2 ) Y_Ado1 &lt;- rbinom(n, size = 1, prob = p_Y_Ado1) p_Y_Ado0 &lt;- dplyr::case_when( Z==1 &amp; A_do_0==1 ~ 0.3, Z==1 &amp; A_do_0==0 ~ 0.6, Z==0 &amp; A_do_0==1 ~ 0.9, Z==0 &amp; A_do_0==0 ~ 0.2 ) Y_Ado0 &lt;- rbinom(n, size = 1, prob = p_Y_Ado0) sim_data &lt;- data.frame(Z, A, Y, Y_Ado1, Y_Ado0) sum(sim_data$Y_Ado1==1)/n sum(sim_data$Y_Ado0==1)/n (sum(sim_data$Y_Ado1==1)/n)-(sum(sim_data$Y_Ado0==1)/n) Phase 2: # Fit a logistic regression model to estimate propensity scores ps_mod &lt;- glm(A ~ Z, data = sim_data, family = &quot;binomial&quot;) # Get the actual propensity scores # predict(..., type = &quot;response&quot;) gives the predicted probabilities from logistic regression # What computations are going on behind the scenes? sim_data$PS &lt;- dplyr::case_when( A==1 ~ predict(ps_mod, type = &quot;response&quot;), A==0 ~ 1-predict(ps_mod, type = &quot;response&quot;) ) # Form inverse probability weights sim_data$weight &lt;- 1/sim_data$PS # Use the IP weights to estimate: # (1) the average outcome if all people were treated # (2) the average outcome if all people were untreated # group_by() forms groups according to the given variable # summarize() computes a summary measure for those groups results &lt;- sim_data %&gt;% group_by(A) %&gt;% summarize(Y_po_estim = sum(Y*weight)/sum(weight)) # Display estimates (1) and (2) results # Compute the estimated average causal effect (ACE) # How does it compare to the truth from &quot;do&quot;ing? diff(results$Y_po_estim) On your own Adapt the simulation to a situation with 2 confounders Z and W as in the DAG below: Note: To simulate dependence on 3 or more variables (e.g., A depends on B, C, and D), an easier approach is to do as below: p_B &lt;- dplyr::case_when( B==1 ~ 0.8, B==0 ~ 0.4 ) p_C &lt;- dplyr::case_when( C==1 ~ 0.9, C==0 ~ 0.5 ) p_D &lt;- dplyr::case_when( D==1 ~ 0.7, D==0 ~ 0.1 ) p_A &lt;- p_B*p_C*p_D If ever D is equal to 1 for all cases, p_A can be updated as: p_A_D1 &lt;- p_B*p_C*0.7 If ever D is equal to 0 for all cases, p_A can be updated as: p_A_D1 &lt;- p_B*p_C*0.1 Questions and Tasks: When you generate \\(A\\), use the following probabilities: p_A &lt;- dplyr::case_when( Z==1 &amp; W==1 ~ 0.3, Z==1 &amp; W==0 ~ 0.6, Z==0 &amp; W==1 ~ 0.9, Z==0 &amp; W==0 ~ 0.2 ) How close are your estimates of the ACE from IP weighting and from graph surgery? Take a careful look at the probabilities used to generate the treatment \\(A\\), and re-examine the form of your propensity score model. Thinking about what the coefficients mean in your propensity score (PS) model, what would you need to do to improve your propensity score model? Fit this new PS model as ps_mod2. Add a second PS variable PS2, and create a corresponding weight2 variable. Store the results from both PS models with something similar to below, Display the estimated ACE resulting from both models. results &lt;- sim_data %&gt;% group_by(A) %&gt;% summarize( Y_po_estim1 = sum(Y*weight1)/sum(weight1), Y_po_estim2 = sum(Y*weight2)/sum(weight2) ) Repeat this simulation using numbers for p_A that make the simpler PS model valid. As before, show the results of both the simpler and more complex PS model. "],
["ip-weighting-in-practice.html", "Topic 6 IP weighting in practice Learning Goals Discussion Analysis", " Topic 6 IP weighting in practice Learning Goals Practice an analysis workflow from start to finish Formulate generalized causal research questions using marginal structural models Interpret results from fitting MSMs Discussion Last time, we derived the following result: If treatment \\(A\\) and outcome \\(Y\\) are d-separated given \\(Z\\) under the null, we can estimate desired “do” probabilities with a weighted average of the outcomes: \\[ P(Y = 1 \\mid \\hbox{do}(A = a)) = \\frac{\\sum_i w_i y_i}{\\sum_i w_i} \\] where the weights \\(w_i\\) are the inverse propensity scores: \\(w_i = 1/P(A = 1 \\mid Z)\\) if \\(a = 1\\) \\(w_i = 1/P(A = 0 \\mid Z)\\) if \\(a = 0\\) When \\(Z\\) contains many variables, we need to model treatment as a function of the variables in \\(Z\\) to obtain estimates of the propensity scores. Can use: Logistic regression Other techniques from Statistical Machine Learning In particular, this weighted average \\[ P(Y = 1 \\mid \\hbox{do}(A = a)) = \\frac{\\sum_i w_i y_i}{\\sum_i w_i} \\] is specific to the situation where \\(Y\\) is a binary outcome variable, and we want to know \\(P(Y = 1)\\) (which also happens to be equal to the expected value of \\(Y\\): \\(E[Y] = P(Y = 1)\\)). Inverse probability weighting works more generally: Treatment \\(A\\) does not have to be binary A continuous treatment is generally difficult to work with Categorical treatments with 3+ categories are naturally handled with extensions of logistic regression (e.g., multinomial regression, other statistical machine learning methods) Outcome \\(Y\\) does not have to be binary Recall from our tree diagrams that inverse probability weighting (see our worksheet with solutions here) upweights the treated to create a population where everyone had been treated upweights the untreated to create a population where everyone had been untreated In these two populations, we counted up the number of instances where \\(Y=1\\) to obtain our “do” probabilities of interest. In essence, the IP weighting created these populations of all treated and all untreated. Considered together, these two populations are called a pseudopopulation because everyone exists twice (as a treated and as an untreated individual). In this pseudopopulation, we essentially fit the simple logistic regression model: \\[ \\log\\hbox{odds}(Y = 1) = \\beta_0 + \\beta_1 A \\] So in summary: we fit a logistic regression model using inverse probability weights. Each individual received the appropriate \\(1/P(A=1\\mid Z)\\) or \\(1/P(A=0\\mid Z)\\) weight. Normally with ordinary unweighted logistic regression, every individual receives a weight of 1. (They appear exactly once in the dataset.) We can make this idea more general: The inverse probability weights effectively repeat certain individuals a number of times so that their data “represents” the other individuals who had received another value of the treatment variable. The IP weights create the pseudopopulation where each individual receives every version of the treatment variable. We can fit any model we want (e.g., linear regression, logistic regression) in this pseudopopulation to relate the outcome to the treatment variable. The process of fitting such a model (i.e., fitting a standard regression model supplemented with IP weights) is the process of fitting marginal structural models New notation: The \\(Y\\) that results when we \\(\\hbox{do}(A=1)\\) can also be notated as \\(Y^{a=1}\\), called the potential outcome under treatment. The \\(Y\\) that results when we \\(\\hbox{do}(A=0)\\) can also be notated as \\(Y^{a=0}\\), called the potential outcome under no treatment. In general the potential outcome when we \\(\\hbox{do}(A=a)\\) is denoted \\(Y^a\\). In words, this potential outcome means: “the outcome that would result if we intervened to set \\(A=a\\)” A marginal structural model (MSM) is a model that relates potential outcomes \\(Y^a\\) to the treatment variable (as opposed to the naturally observed outcomes \\(Y\\)): A linear regression MSM for continuous/quantitative outcomes \\(Y\\): \\[ E[Y^a] = \\beta_0 + \\beta_1 a \\] A logistic regression MSM for binary outcomes \\(Y\\): \\[ \\log\\left( \\frac{P(Y^a = 1)}{1-P(Y^a = 1)} \\right) = \\beta_0 + \\beta_1 a \\] Where does the name “marginal structural model” come from? Marginal: refers to the fact that the marginal mean (as opposed to a conditional mean, which is a mean in a subgroup) is being modeled (much like how it’s used in “marginally dependent/independent”) Structural: In general models in causal inference that describe potential outcomes are referred to as “structural” to indicate a describing of relationships beyond only associations Analysis A template Rmd is available here. Our data come from the National Health and Nutrition Examination Survey Data I Epidemiologic Follow-up Study (NHEFS). library(readr) library(dplyr) library(ggplot2) nhefs &lt;- read_csv(&quot;https://cdn1.sph.harvard.edu/wp-content/uploads/sites/1268/1268/20/nhefs.csv&quot;) Download the codebook for the dataset here, and have it open for reference. Research goal: What is the average causal effect of smoking cessation on weight gain at a follow-up visit about 10 years later? Part 1: Getting a feel for the data Tabulate the treatment variable qsmk, visualize the outcome variable, and get summary statistics for the outcome variable. How many missing values are there for these variables? What key implication does this have for our DAG? # Tabulate with the count() function from dplyr # count() does show NAs, if present nhefs %&gt;% count(qsmk) # Construct a visualization of the weight gain variable # Summary statistics for the weight gain variable summary(nhefs$wt82_71) We will need to exclude cases with missing data for the outcome. Work with the nhefs_subs for the remainder of this analysis: nhefs_subs &lt;- nhefs %&gt;% filter(!is.na(wt82_71)) Let’s look at the variable distributions within the treated (qsmk = 1: the quitters) and the untreated (qsmk = 0). # Here we enumerate the quantitative and the categorical variables quant_vars &lt;- c(&quot;age&quot;, &quot;alcoholhowmuch&quot;, &quot;cholesterol&quot;, &quot;ht&quot;, &quot;pregnancies&quot;, &quot;price71&quot;, &quot;smokeintensity&quot;, &quot;smokeyrs&quot;, &quot;tax71&quot;, &quot;wt71&quot;) categ_vars &lt;- c(&quot;active&quot;, &quot;alcoholfreq&quot;, &quot;alcoholpy&quot;, &quot;alcoholtype&quot;, &quot;allergies&quot;, &quot;asthma&quot;, &quot;boweltrouble&quot;, &quot;bronch&quot;, &quot;chroniccough&quot;, &quot;colitis&quot;, &quot;diabetes&quot;, &quot;education&quot;, &quot;exercise&quot;, &quot;hayfever&quot;, &quot;hbp&quot;, &quot;hbpmed&quot;, &quot;headache&quot;, &quot;hepatitis&quot;, &quot;hf&quot;, &quot;hightax82&quot;, &quot;income&quot;, &quot;infection&quot;, &quot;lackpep&quot;, &quot;marital&quot;, &quot;nerves&quot;, &quot;nervousbreak&quot;, &quot;otherpain&quot;, &quot;pepticulcer&quot;, &quot;pica&quot;, &quot;polio&quot;, &quot;race&quot;, &quot;school&quot;, &quot;sex&quot;, &quot;tb&quot;, &quot;tumor&quot;, &quot;weakheart&quot;, &quot;wtloss&quot;) # Compare the means of the quantitative variables in the treated and untreated nhefs_subs %&gt;% group_by(qsmk) %&gt;% summarize_at(.vars = quant_vars, .funs = mean) %&gt;% as.data.frame() # Compare the distributions of the categorical variables in the treated and untreated # First row numbers are P(covariate | qsmk=0) # Second row numbers are P(covariate | qsmk=1) for (c_var in categ_vars) { cat(c_var, &quot;:\\n&quot;) table(qsmk = nhefs_subs$qsmk, nhefs_subs[[c_var]], useNA = &quot;ifany&quot;) %&gt;% prop.table(margin = 1) %&gt;% print() cat(&quot;\\n&quot;) } How could you use these data summaries to help with DAG construction (coming next)? Part 2: Causal DAG construction Building the causal DAG from start to finish would normally be a much longer process. So that we can get to the data analysis practice, we’ll abbreviate this crucial step. We’ll start with the following d-separating set: sex age race education smokeintensity smokeyrs active exercise wt71 Open up the web version of DAGitty, which we’ll use to draw our causal diagram. In the top menu bar, click “Model” &gt; “New model”. Also go through the “How to …” Add the treatment and outcome nodes as well as the above variables. Also add the relevant edges. Looking briefly through the codebook and your data summaries above, do you think that there are other key variables to include in the DAG? There is potential selection bias at play. (There were missing values for the response variable.) Add a selection node and give it the “adjusted” status (under the left “Variable” menu). What factors do you think are related to not showing up for the second study visit (and thus not having weight gain measured)? Based on this add edges between the selection node and other variables. What is your d-separating set? So that you can come back to your DAG easily, copy and paste the “Model code” in the right side menu into a separate text document. Part 3: Propensity score modeling In order to fit marginal structural models (MSMs) that allow us to estimate average causal effects, we need to estimate the propensity scores. Make visualizations to inform the nature of the relationship between treatment and the quantitative predictors. For example, you can use code like the following to see if the observed probabilities (in blue) match up with those predicted by a logistic regression model with a quadratic function of age (in red). The y~poly(x,2) could be changed to y~x to see the results of having age linearly related to the log odds of treatment. State your conclusions from these visualizations. ggplot(nhefs, aes(x = age, y = qsmk)) + geom_point() + geom_smooth(method = &quot;loess&quot;, se = FALSE, color = &quot;blue&quot;) + geom_smooth(formula = y~poly(x,2), method=&quot;glm&quot;, method.args=list(family=&quot;binomial&quot;), se = FALSE, color = &quot;red&quot;) Based on your visualizations, construct an appropriate propensity score model. To include polynomial relationships in the model you can use +poly(age, 2), for example, in the model formula. Compute the estimated propensity scores and inverse probability weights as we did in our simulations. Call the weight variable weight1. Here you’ll want to check if nhefs_subs$qsmk==1 or nhefs_subs$qsmk==0. Store the IP weights in your nhefs_subs dataset. Part 4: Fitting marginal structural models Install the geepack package which we’ll be using to fit MSMs. This package fits linear and logistic regression models like lm() and glm() do, but it uses a better estimate of the standard error in the presence of weights. The code below fits the MSM: \\[ E[Y^a] = \\beta_0 + \\beta_1 \\hbox{qsmk} \\] library(geepack) msm_fit1 &lt;- geeglm( wt82_71 ~ qsmk, data = nhefs_subs, weights = weight1, id = seqn, corstr = &quot;independence&quot; ) summary(msm_fit1) Interpret both the intercept and qsmk coefficients. Confidence intervals are not displayed in the output, so we’ll compute them by hand. (p-values are displayed, but it’ll be nice to have confidence intervals because they nicely convey both the estimate itself and its uncertainty.) The coefficient estimates are expected to be normally distributed with mean equal to the true population value and standard deviation equal to the standard error. Use the qnorm() function to obtain the 95% confidence interval for the qsmk coefficient. Summarize your results from this analysis. Adapt the above code to fit an effect modification MSM: \\[ E[Y^a] = \\beta_0 + \\beta_1\\,\\hbox{qsmk} + \\beta_2\\,\\hbox{sex} + \\beta_3\\,\\hbox{qsmk}\\times \\hbox{sex} \\] Interpret both the qsmk and interaction coefficients. Compute confidence intervals for those coefficients, and summarize your results. "],
["ip-weighting-for-censoring.html", "Topic 7 IP weighting for censoring Learning Goals Discussion Exercises", " Topic 7 IP weighting for censoring Learning Goals Understand how IP weighting can be used to address the lack of generalizability caused by censoring Discussion Language for interpreting coefficients in MSMs \\[ E[Y^a] = \\beta_0 + \\beta_1 a \\] Context: Y is cholesterol, A is medication (a = 1: on medication) \\(\\beta_0\\): The average potential cholesterol level when all are not on medication \\(\\beta_1\\): The average causal effect: the change in average potential cholesterol level if everyone were on medication compared to if no one were on medication Do we really care about the ACE? Depends on research goals, but yes, sometimes ACE is of interest. Is it a nationwide policy? If so, then enacting the policy means “treating” everyone and not enacting means treating no one. May care about the ACE within subgroups. We’ve seen this with effect modification MSMs: \\[ E[Y^a] = \\beta_0 + \\beta_1 a + \\beta_2 M + \\beta_3 a\\times M \\] Say \\(M\\) indicates history of heart disease (1: history. 0: no history) Question: What are the interpretation of \\(\\beta_1\\) and \\(\\beta_3\\) in this model? What is the difference between regular and IP-weighted GLMs? Our tree diagrams showed that weighting allows us to create “populations” of all treated and all untreated. Our WHATIF book calls the combined population of the all treated and the all untreated a pseudopopulation We can model the outcomes in this pseudopopulation with standard regression models (marginal structural models). Allows us to estimate causal quantities: \\(P(Y \\mid \\hbox{do}(A=a))\\), \\(E[Y \\mid \\hbox{do}(A=a)]\\) (in potential outcome notation: \\(P(Y^a)\\), \\(E[Y^a]\\)) Standard regression models without IP weights do not estimate these causal quantities. Censoring and selection bias Reminder: NHEFS investigation from last time Research goal: What is the average causal effect of smoking cessation on weight gain at a follow-up visit about 10 years later? There were some people who did not have their weight measured at Visit 2. They must be excluded from our analysis. Define a censoring indicator \\(C\\) where \\(C = 1\\) means censored/excluded and \\(C = 0\\) means uncensored/included. Our analysis only generalizes to uncensored individuals–the types of people who would stay in the study! Before: just wanted to find \\(Z\\) such that conditional on \\(Z\\) (within subsets defined by \\(Z\\)), the outcomes of the treated and untreated were comparable. Could appropriately upweight the treated and untreated to get “populations” of all treated and all untreated. Now: also want to ensure that the outcomes in the censored and uncensored are comparable. Can appropriately upweight the uncensored to represent everyone had they not been censored. The censored get zero weight. (We don’t observe them!) When are the outcomes in the censored and uncensored comparable? When variables in \\(A\\) and \\(Z_2\\) together d-separate \\(C\\) and the outcome \\(Y\\). e.g., The DAG below. View \\(C\\) as the new “treatment” variable. Another way to put it: when we put a box around \\(C\\) in the DAG, \\(Z\\) must d-separate \\(A\\) and \\(Y\\) under the null. Same condition as before, now explicit about paths with conditioned-on colliders (non-backdoor paths). Questions: \\(C\\) is inherently conditioned on in our analysis (\\(C = 0\\)). What set \\(Z\\) d-separates the treatment and outcome under the null? What is a set \\(Z_2\\) such that \\(Z_2 \\cup A\\) (read \\(Z_2\\) “union” \\(A\\), or \\(Z_2\\) and \\(A\\) together) d-separate \\(C\\) and the outcome? Conclusion: \\(Z\\) should include: sex, age, race, education, smokeintensity, smokeyrs, active, exercise, wt71, and VidGames (Same as before with the addition of VidGames) \\(Z\\) blocks spurious, non-causal paths between \\(A\\) (qsmk) and \\(Y\\) (wt82_71) \\(Z_2\\) should include VidGames Together with \\(A\\), \\(Z_2\\) blocks spurious, non-causal paths between \\(C\\) and \\(Y\\) Note that \\(Z_2\\) is contained within \\(Z\\). This will be the case when \\(A\\) is a cause of \\(C\\) (often true). Ultimate goal: Want to upweight individuals using \\(P(A, C = 0 \\mid Z)\\) Why? Same rationale as our tree diagram with additional \\(C = 1\\) and \\(C = 0\\) branches. Allows us to estimate \\(E[Y^{a, c = 0}]\\): expected potential outcome under treatment \\(a\\) and when no one has been censored (\\(C = 0\\)) We can express \\(P(A, C = 0 \\mid Z)\\) as the following: \\[ \\begin{align*} P(A, C = 0 \\mid Z) &amp;= P(A, C = 0, Z)/P(Z) \\\\ &amp;= P(A \\mid Z) P(C = 0 \\mid A, Z) \\end{align*} \\] Based on the last line, we see (1) the propensity score and (2) another “propensity” of being uncensored. Will want models for both. (The censoring probability is not typically called a propensity score.) A little extra theory (don’t worry about the math part below if you haven’t taken Probability): \\(Z\\) may contain extra variables not in \\(Z_2\\) if there are, say, many common causes of \\(A\\) and \\(Y\\), but only a few of these are a cause of \\(C\\). Let’s say that these extra variables are in the set \\(Z_1\\). (\\(Z = Z_1 \\cup Z_2\\)) If the variables in \\(Z_1\\) are not causes of \\(C\\), then \\(C\\) and \\(Z_1\\) are conditionally independent given \\(A\\) and \\(Z_2\\). Going through the work below… \\[ \\begin{align*} P(A, C = 0 \\mid Z) &amp;= P(A, C = 0 \\mid Z_1, Z_2) \\\\ &amp;= P(A, C = 0, Z_1, Z_2)/P(Z_1, Z_2) \\\\ &amp;= P(A \\mid Z_1, Z_2) P(C = 0 \\mid A, Z_1, Z_2) \\\\ &amp;= P(A \\mid Z_1, Z_2) P(C = 0 \\mid A, Z_2) \\end{align*} \\] …this implies that the propensity score model (for \\(A\\)) should depend on all variables in \\(Z\\) (the set \\(Z_1\\) and \\(Z_2\\) together) and that the censoring probability model only need depend on \\(Z_2\\). Analysis plan: Notation: \\(Z\\) is the set that d-separates \\(A\\) and \\(Y\\) when \\(C\\) is conditioned on. \\(Z_2\\) (a subset of \\(Z\\)) that, together with \\(A\\), d-separates \\(C\\) and \\(Y\\) Model the propensity scores \\(P(A\\mid Z)\\) using all variables \\(Z\\) in the d-separating set. Create inverse probability weights \\(W^A = 1/P(A\\mid Z)\\) (treatment weights). Model the probability of censoring \\(P(C \\mid A, Z_2)\\). Create inverse probability weights \\(W^C = 1/P(C = 0\\mid A, Z_2)\\) (censoring weights). Create final weights (for treatment and censoring) \\(W^{A,C} = W^A\\times W^C\\). Fit the desired marginal structural model with these final weights. Exercises A template Rmd is available here. Setup library(readr) library(dplyr) library(ggplot2) library(geepack) nhefs &lt;- read_csv(&quot;https://cdn1.sph.harvard.edu/wp-content/uploads/sites/1268/1268/20/nhefs.csv&quot;) Create a censoring indicator variable that will be TRUE if the individual was censored and FALSE otherwise. nhefs$cens &lt;- is.na(nhefs$wt82_71) Throughout we’ll use the full nhefs dataset, and we’ll work from the DAG below (\\(C\\) has been conditioned on): Part 1: Modeling to obtain weights Based on the DAG above, identify the set of variables \\(Z\\) that d-separates qsmk and weight gain. Fit an appropriate treatment propensity score model, and call this ps_treat_mod. For simplicity, assume that a quadratic relationship for the quantitative variables gives a good fit. Make sure to wrap categorical variables inside factor() in your model formula. Add a new variable to your dataset called weights_treat that contains the IP weights for treatment. Is it believable that quitting smoking (treatment) is a cause of censoring? How do the tabulations/calculations below help answer this? # P(censored | quitters) and P(censored | nonquitter) table(qsmk = nhefs$qsmk, cens = nhefs$cens) table(qsmk = nhefs$qsmk, cens = nhefs$cens) %&gt;% prop.table(margin = 1) How would you see if the data support the age --&gt; C and education --&gt; C arrows? Describe, but don’t actually perform this analysis. Fit an appropriate model for the probability of censoring, and call this prob_cens_mod. For simplicity, assume that a quadratic relationship for the quantitative variables gives a good fit. Make sure to wrap categorical variables inside factor() in your model formula. Add a new variable to your dataset called weights_cens that contains the IP weights for censoring. (Note that in R, the default for logistic regression is to model the probability of the outcome variable equaling 1.) Add a final weight variable to your dataset called weight_TC. Part 2: Fitting MSMs Fit the following two MSMs (\\(A\\) is qsmk, and for sex, 0 indicates males and 1 indicates females): \\[ E[Y^{a, c=0}] = \\beta_0 + \\beta_1 a \\] \\[ E[Y^{a, c=0}] = \\beta_0 + \\beta_1 a + \\beta_2\\hbox{sex} + \\beta_3 a\\times\\hbox{sex} \\] Note: In our previous analysis where we worked with nhefs_subs, we had actually fit the following: \\[ E[Y^{a} \\mid C = 0] = \\beta_0 + \\beta_1 a \\] \\[ E[Y^{a} \\mid C = 0] = \\beta_0 + \\beta_1 a + \\beta_2\\hbox{sex} + \\beta_3 a\\times\\hbox{sex} \\] Refit these models here using the relevant weights. How do your results compare? "],
["causal-discovery.html", "Topic 8 Causal Discovery Learning Goals Discussion Exercises", " Topic 8 Causal Discovery Learning Goals Understand when causal discovery might be useful Understand the ideas underlying causal discovery algorithms Predict how algorithm output is affected by tuning parameters Extend core ideas of discovery algorithms to new situations Discussion Slides are available here. Exercises Exercise 1 The SGS algorithm will find the correct equivalence class of graphs but is quite slow. We’ll build up the ideas underlying a faster algorithm, the PC algorithm (stands for Peter-Clark). Consider the testing of whether or not two variables are conditionally independent. How would you expect the computing time of this test to change as the variable set being conditioned on increases? Explain. Recall the edge deletion step of SGS: For every pair of variables \\(X\\) and \\(Y\\) and all other sets \\(Z\\) of other variables, check if we can make \\(X \\perp \\!\\!\\! \\perp Y \\mid Z\\). If so, remove the edge between \\(X\\) and \\(Y\\). What strategies might we employ to make this step faster? What might be a better order in which to test these sets \\(Z\\)? What statistical advantages are linked to the better order you decided on in (b)? Think in terms of statistical power. Exercise 2 It’ll be handy to have the TETRAD manual open. Phase 1: Simulate data for a chain X -&gt; Y -&gt; Z and for a collider X -&gt; Y &lt;- Z. For each, write the data to a CSV file using the following. (Assuming that you will save this data in a folder called tetrad.) write.csv(sim_data, file = &quot;tetrad/chain_binary.csv&quot;) write.csv(sim_data, file = &quot;tetrad/collider_binary.csv&quot;) Phase 2: Exploration of TETRAD functionality as a class. Phase 3: In groups, explore select aspects of the parameters window: Collider discovery: experiment with options 2 (CPC, the default) and 3 (Max-P) Cutoff for p-values Exercise 3 There are two genes (Gene A and Gene B) that produce Protein X. Gene A is the primary producer. Whenever Gene A is functional, Gene B is inactive and produces nothing. However, if Gene A loses function, Gene B becomes active and produces Protein X in Gene A’s place in exactly the same amounts. Draw the DAG implied by this expert knowledge. We can view Gene A as a binary variable with values “functional” and “non-functional”. Will Gene A and Protein X be marginally independent or marginally dependent in the data? Discuss your answers to (a) and (b) in the context of a relevant concept. Exercise 4 In this exercise, we’ll think about the conditional independence test and the role of the p-value cutoff parameter. In what phase(s) of causal discovery algorithms do we need to test for conditional (in)dependence? These conditional independence tests need to test statements of the form: \\(X \\perp \\!\\!\\! \\perp Y \\mid Z\\). Describe how regression models could test this. Make sure to clearly describe the null hypothesis. As the p-value cutoff is lowered to 0, what would you expect to happen to the graph pattern learned by causal discovery algorithms? As the p-value cutoff is increased to 1? Exercise 5 The algorithms we’ve discussed so far are called constraint-based methods because they test conditional independence constraints. Another class of methods are called score-based methods. Read over a description of the Greedy Equivalence Search algorithm here, and provide a high-level overview of how the algorithm works in your own words. Exercise 6 Consider data that truly come from a chain X -&gt; Y -&gt; Z. Step through the full causal discovery algorithm and report the output that it would give. Suppose you could give background knowledge on just one edge that is required to be present. (Perhaps you have temporal information on those variables.) What edge would allow the entire structure to be learned? Exercise 7 Consider data that truly come from a fork X &lt;- Y -&gt; Z. What output would a causal discovery algorithm give? What type of background information could be given to help uncover the true structure? A required edge? Temporal information? Exercise 8 The PC algorithm assumes that the set of observed variables is the complete set present in the graph. That is, it assumes no unmeasured (latent) variables. How could we modify the PC algorithm to account for latent variables? "],
["sensitivity-analyses-for-unmeasured-confounding.html", "Topic 9 Sensitivity Analyses for Unmeasured Confounding Learning Goals Exercises", " Topic 9 Sensitivity Analyses for Unmeasured Confounding Learning Goals Understand the role of sensitivity analyses in sound scientific practice Implement and interpret the results of sensitivity analyses Exercises A template Rmd is available here. library(ggplot2) library(dplyr) library(geepack) Given the code below, what is the underlying DAG? set.seed(302) n &lt;- 5000 Z &lt;- rbinom(n, size = 1, prob = 0.5) p_A &lt;- dplyr::case_when( Z==1 ~ 0.6, Z==0 ~ 0.2 ) A &lt;- rbinom(n, size = 1, prob = p_A) Y &lt;- 0.1*A + 0.3*Z + rnorm(n) sim_data &lt;- data.frame(A, Y, Z) # Create a subject ID variable sim_data$subj_id &lt;- seq_len(n) First outline the process for fitting a marginal structural model to estimate the average causal effect of treatment \\(A\\) on outcome \\(Y\\). Then write the relevant code. You will need to use subj_id as the id variable within geeglm(). # Original MSM fit Here we begin our sensitivity analysis. We’ll create a binary unmeasured variable \\(U\\) that has varying degrees of association with both \\(A\\) and \\(Y\\). Add extra comments throughout the code to better document this implementation of a sensitivity analysis. Complete the sensitivity analysis by completing the TASK referred to within the for-loop. # Create a vector of possible associations that U has with A and Y U_assocs &lt;- seq(0.05,2,0.1) # Create empty storage containers to store the new ACEs and CI bounds new_aces &lt;- rep(0, length(U_assocs)) new_ci_upper &lt;- rep(0, length(U_assocs)) new_ci_lower &lt;- rep(0, length(U_assocs)) # Loop over possible values of the association for (i in seq_along(U_assocs)) { U_assoc &lt;- U_assocs[i] log_odds_U &lt;- U_assoc*sim_data$Y + U_assoc*sim_data$A p_U &lt;- exp(log_odds_U)/(1+exp(log_odds_U)) U &lt;- rbinom(n, size = 1, prob = p_U) sim_data$U &lt;- U # TASK: Fit the new MSM that accounts for U new_msm_fit &lt;- ??? # Store relevant values new_aces[i] &lt;- summary(new_msm_fit)$coefficients[&quot;A&quot;, &quot;Estimate&quot;] se &lt;- summary(new_msm_fit)$coefficients[&quot;A&quot;, &quot;Std.err&quot;] new_ci_upper[i] &lt;- new_aces[i]+(qnorm(0.975)*se) new_ci_lower[i] &lt;- new_aces[i]-(qnorm(0.975)*se) } Below we plot the results of this sensitivity analysis. The black line shows the different ACEs, and the ribbon shows the corresponding 95% confidence interval. The red line shows the original ACE estimate. What information can be gained from this plot? What is the smallest magnitude of association with \\(U\\) that leads to qualitatively different conclusions? Look back to where we simulated \\(Y\\). How could we change this so that the smallest magnitude referred to in the previous question is bigger? sens_data &lt;- data.frame(lnOR = U_assocs, ace = new_aces, ci_lower = new_ci_lower, ci_upper = new_ci_upper) ggplot(sens_data, aes(x = lnOR, y = ace)) + geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper), fill = &quot;grey70&quot;) + geom_line() + geom_hline(aes(yintercept = summary(orig_msm_fit)$coefficients[&quot;A&quot;, &quot;Estimate&quot;]), color = &quot;red&quot;) Closing questions: How could we adapt this structure for a sensitivity analysis to situations where the variables under consideration are of different types (quantitative vs. categorical) than what we considered here? How could these sensitivity analyses be used for assessing the impact of even more general structures of unmeasured confounding? "],
["graphical-structure-of-mediation.html", "Topic 10 Graphical Structure of Mediation Learning Goals Warm-up Exercises", " Topic 10 Graphical Structure of Mediation Learning Goals Understand scientific motivations for mediation analysis and pose research questions that can be investigated by direct and indirect effects. Describe how regression models can be used to estimate direct and indirect effects. Apply d-separation ideas to understand the graphical reasoning behind the confounding assumptions needed to identify direct and indirect effects. Warm-up In your groups, discuss responses to the video questions corresponding to the Mediation Analysis video. Exercises Background Consider the following causal diagram representing variables important in mediation analysis: Consider the following 4 assumptions: No unmeasured confounding of the treatment-outcome relationship (\\(A\\) and \\(Y\\)). No unmeasured confounding of the mediator-outcome relationship (\\(M\\) and \\(Y\\)). No unmeasured confounding of the treatment-mediator relationship (\\(A\\) and \\(M\\)). No confounder of the mediator-outcome relationship is affected by treatment (arrows from \\(A\\) to \\(C_2\\)). When these 4 assumptions hold, the CDE, NDE, and NIE are identifiable, and we can use models such as the regression models below to estimate them. (\\(C = \\{C_1, C_2, C_3\\}\\)) \\[ E[Y\\mid A, M, C] = \\theta_0 + \\theta_1 A + \\theta_2 M + \\theta_3 AM + \\mathbf{\\theta_4&#39;C} \\] \\[ E[M\\mid A, C] = \\beta_0 + \\beta_1 A + \\mathbf{\\beta_2&#39;C} \\] Identifiability: A quantity (here, the different direct and indirect effects) is identifiable if the true value is able to be learned from an infinite amount of data. In causal inference, threats to identifiability are usually the result of unmeasured variables that create undesired non-causal paths. In the later exercises, it will be helpful to refer to the following list of paths from \\(A\\) to \\(Y\\): ## Path 1: A &lt;- C1 -&gt; Y ## Path 2: A -&gt; Y ## Path 3: A &lt;- C3 -&gt; M -&gt; Y ## Path 4: A &lt;- C3 -&gt; M &lt;- C2 -&gt; Y ## Path 5: A -&gt; M -&gt; Y ## Path 6: A -&gt; M &lt;- C2 -&gt; Y ## Path 7: A -&gt; C2 -&gt; Y ## Path 8: A -&gt; C2 -&gt; M -&gt; Y Exercise 1 Assumptions 1 and 2 are needed in order to identify the controlled direct effect (CDE). We will use the above DAG to understand why. First argue that drawing a box around \\(M\\) in the DAG is relevant here. Using d-separation ideas, argue why Assumptions 1 and 2 must hold. Using d-separation ideas, argue why it is not necessary for Assumptions 3 and 4 to hold if 1 and 2 hold. Exercise 2 In addition to Assumptions 1 and 2, Assumptions 3 and 4 are also needed to identify the natural effects (the NDE and NIE). Using d-separation ideas, why must Assumption 1 still hold to identify the NDE and NIE? Now let’s look at Assumption 2. Argue that the same d-separation reasoning from Exercise 1 applies for understanding why Assumption 2 must hold to identify the NDE. The natural indirect effect can be viewed as a composition of the \\(A\\) to \\(M\\) effect and the \\(M\\) to \\(Y\\) effect. Given this, why must Assumption 2 hold? Now let’s look at Assumption 3. How do the two natural effects differ from the controlled direct effect? Given this, why does unmeasured treatment-mediator confounding pose a concern? Assumption 4 is harder to justify purely graphically, but if you are curious about it and the underlying proof, ask the instructor. Exercise 3 We can use models such as the regression models below to estimate the CDE, NDE, and NIE. (\\(C = \\{C_1, C_2, C_3\\}\\), and \\(\\theta_4, \\beta_2\\) are vectors of coefficients.) \\[ E[Y\\mid A, M, C] = \\theta_0 + \\theta_1 A + \\theta_2 M + \\theta_3 AM + \\theta_4&#39;C \\] \\[ E[M\\mid A, C] = \\beta_0 + \\beta_1 A + \\beta_2&#39;C \\] Given these models, show that the 3 effects are given by: CDE: \\(\\theta_1 + \\theta_3 m\\) NDE: \\(\\theta_1 + \\theta_3 (\\beta_0 + \\beta_2&#39;C)\\) NIE: \\(\\beta_1(\\theta_2 + \\theta_3)\\) Note: Next time, we’ll explore how this approach generalizes to more flexible ways to estimate mediation effects. "],
["applied-mediation-analysis.html", "Topic 11 Applied Mediation Analysis Learning Goals Discussion Exercises", " Topic 11 Applied Mediation Analysis Learning Goals Understand a simulation approach for estimating effects of interest in mediation analysis Learn to use the mediation R package Use mediation analysis to answer research questions Draw connections to the framework of sensitivity analyses for unmeasured confounding Discussion What is the simulation approach for estimating mediation effects? Parametric approach: assume we know the distribution of key quantities Nonparametric approach (bootstrapping): we don’t assume we know the distribution of key quantities If key assumptions hold, we can use models to estimate mediation effects: \\(E[Y\\mid A, M, C]\\) \\(E[M\\mid A, C]\\) Essentially, these models are structural: they describe average potential outcomes. If these models have an explicit and convenient form (like below)… \\[ E[Y\\mid A, M, C] = \\theta_0 + \\theta_1 A + \\theta_2 M + \\theta_3 AM + \\theta_4&#39;C \\] \\[ E[M\\mid A, C] = \\beta_0 + \\beta_1 A + \\beta_2&#39;C \\] …there are formulas for the average mediation effects. (See Exercise 3 from Topic 10.) But what if the \\(E[Y\\mid A, M, C]\\) and \\(E[M\\mid A, C]\\) models don’t allow for ready formulas? Situation 1: Parametric Approach Key feature: We assume that we know the true sampling distribution of the parameter estimates from the \\(E[Y\\mid A, M, C]\\) and \\(E[M\\mid A, C]\\) models. Fit the \\(E[Y\\mid A, M, C]\\) and \\(E[M\\mid A, C]\\) models. (Only do this once.) Use the sampling distribution of the parameter estimates to randomly draw (simulate) a set of parameters. (Goal: will do this many times to quantify the sampling variability of our mediation effect estimates.) These randomly drawn parameters define the \\(Y\\) and \\(M\\) models. For each individual: Simulate \\(K\\) copies of \\(M\\) under \\(A = 0\\) and \\(A = 1\\) using this individual’s \\(C\\) values. Use the simulated \\(M\\) values to simulate the \\(Y\\) values under both \\(A = 0\\) and \\(A = 1\\). Average the potential outcomes over the K copies. This provides an estimate of the individual-level potential outcomes. Average the individual-level potential outcomes. End up with one estimate each of \\(E[Y^{1M_1}], E[Y^{1M_0}], E[Y^{0M_1}], E[Y^{0M_0}]\\) Subtract the appropriate values to obtain direct and indirect effects. Repeat step 2 \\(B \\geq 1000\\) times. Finally, summarize: Average the \\(B\\) copies of the effect estimates to get one overall estimate for each effect. Use the 2.5th and 97.5th percentiles to obtain a 95% confidence interval. Situation 2: Nonparametric Approach Resample the original dataset with replacement, maintaining the original sample size. Using the resampled data, fit the \\(E[Y\\mid A, M, C]\\) and \\(E[M\\mid A, C]\\) models. We use the models to simulate \\(M\\) and \\(Y\\). For each individual: Simulate \\(K\\) copies of \\(M\\) under \\(A = 0\\) and \\(A = 1\\) using this individual’s \\(C\\) values. Use the simulated \\(M\\) values to simulate the \\(Y\\) values under both \\(A = 0\\) and \\(A = 1\\). Average the potential outcomes over the K copies. This provides an estimate of the individual-level potential outcomes. Average the individual-level potential outcomes. End up with one estimate each of \\(E[Y^{1M_1}], E[Y^{1M_0}], E[Y^{0M_1}], E[Y^{0M_0}]\\) Subtract the appropriate values to obtain direct and indirect effects. Repeat steps 1 and 2 \\(B \\geq 1000\\) times. Finally, summarize: Average the \\(B\\) copies of the effect estimates to get one overall estimate for each effect. Use the 2.5th and 97.5th percentiles to obtain a 95% confidence interval. Exercises A template Rmd is available here. Install the mediation package: install.packages(&quot;mediation&quot;) Data context We’ll look at data from a study (cited below) of how the framing of immigration issues in a video affects attitudes about immigration. Brader, T., Valentino, N. and Suhay, E. (2008). What triggers public opposition to immigration? Anxiety, group cues, and immigration threat. American Journal of Political Science 52, 4, 959–978. In this study authors constructed different videos of a news story about immigration, and the video was framed and portrayed in 4 different ways. Study participants were randomly assigned to watch one of the 4 videos and were subsequently asked questions related to their stances on immigration. Outcome variables: (We’ll focus on cong_mesg.) immigr: A four-point scale measuring subjects’ attitudes toward increased immigration. Larger values indicate more negative attitudes. english: A four-point scale indicating whether subjects favor or oppose a law making English the official language of the U.S. cong_mesg: Whether subjects requested sending an anti-immigration message to Congress (1 = yes, 0 = no). anti_info: Whether subjects wanted to receive information from anti-immigration organizations (1 = yes, 0 = no). Treatment variables: (We’ll focus on treat.) tone: 1st treatment; whether the news story is framed positively (0) or negatively (1). eth: 2nd treatment; whether the news story features a European (0) or Latino (1) immigrant. treat: Product of the two treatment variables. 1 if the news story was framed negatively AND about Latino immigrants. 0 otherwise. Mediator variables: (We’ll focus on emo and p_harm.) emo: Measure of subjects’ negative feeling during the experiment. A numeric scale ranging between 3 and 12 where 3 indicates the most negative feeling. anx: A four-point scale measuring subjects’ anxiety about increased immigration. p_harm: Subjects’ perceived harm caused by increased immigration. A numeric scale between 2 and 8. Confounders: age: Subjects’ age. educ: Subjects’ highest educational attainments. gender: Subjects’ gender. income: Subjects’ income, measured as a 19-point scale. Load required packages and the data as below: library(mediation) library(MASS) library(dplyr) library(ggplot2) data(framing) Research question: Is the effect of the framing treatment mediated more by negative emotions (emo) or by perceived harm (p_harm)? Exercise 1 The causal diagram underlying this investigation is below: This study was a randomized experiment, in which participants were assigned to the different treatment groups. Given this, what do you notice about the DAG structure, and why does this make sense? In the context of this study, what assumptions are we making when we use modeling to estimate direct and indirect effects? (Phrase this in terms of the specific variables in the study. This will help with the next part.) After thinking about the assumptions we’re making, do you agree with this DAG? Draw an updated DAG reflecting your expert knowledge using DAGitty, and include the DAG in your document. (You can start from the DAGitty code above. Copy and paste the dag {...} part into the “Model code” pane on the right.) Exercise 2 Despite your excellent updated expert knowledge DAG, we’ll have to proceed with the measured variables (i.e., in the context of the original DAG shown above). Fit an appropriate model for the mediator (emo), and call this model med_mod. Fit an appropriate model for the outcome (cong_mesg), and call this model out_mod. Make sure to use visualizations to specify an appropriate form for the model. Recall that you can add code like below to your plots to show observed trends in blue and predicted trends from different model formulations in red: ## For a quantitative outcome: + geom_smooth(method = &quot;loess&quot;, se = FALSE, color = &quot;blue&quot;) + geom_smooth(formula = y~poly(x,2), method=&quot;lm&quot;, se = FALSE, color = &quot;red&quot;) ## For a binary outcome: + geom_smooth(method = &quot;loess&quot;, se = FALSE, color = &quot;blue&quot;) + geom_smooth(formula = y~poly(x,2), method=&quot;glm&quot;, method.args=list(family=&quot;binomial&quot;), se = FALSE, color = &quot;red&quot;) Exercise 3 With the mediator and outcome models fit, we can use the core function in the mediation package: mediate(). The code below performs the simulation method to estimate mediation effects with \\(B = 1000\\). set.seed(394) mediate_out &lt;- mediate(med_mod, out_mod, treat = &quot;treat&quot;, mediator = &quot;emo&quot;, robustSE = TRUE, sims = 1000) summary(mediate_out) There’s a lot in this output! When learning a new package (a great skill to have!), it is essential to visit the documentation. The landing page for the package will always have a reference manual containing the documentation for individual functions, but more sometimes, a more useful vignette will be available. Open the vignette titled “mediation” (not “mediation-old”) and read the third page describing how different effects are defined. The notation is a little different than what we have defined - if you have questions, ask the instructor. Which of the ACMEs (control or treated) corresponds to an effect we have defined? Which effect is it? Give an interpretation of this measure. (Note: these effects are expressed as probability differences.) Based on the confidence interval, do we have evidence for a true effect in the broader population? Which of the ADEs (control or treated) corresponds to an effect we have defined? Which effect is it? Give an interpretation of this measure. Based on the confidence interval, do we have evidence for a true effect in the broader population? How large is the NIE relative to the total effect and how does this move us toward answering our research question? Exercise 4 Now perform a mediation analysis using p_harm as the mediator of interest. This includes: Fitting appropriate mediator and outcome models Using the mediate() function to estimate the mediation effects Cite relevant numbers from the mediate() output to answer our original research question. Research question: Is the effect of the framing treatment mediated more by negative emotions or by perceived harm? Exercise 5 Let’s not forget about your improved expert DAG. Describe in detail (but don’t actually implement) how we could implement a sensitivity analysis to assess sensitivity of the above results to unmeasured confounding. Exercise 6 The controlled direct effect is not estimated by this package! Identify the specific place where the simulation approach could be changed to estimate the CDE. "],
["rct-and-iv-designs.html", "Topic 12 RCT and IV designs Learning Goals Warm-up Discussion Exercises", " Topic 12 RCT and IV designs Learning Goals Use d-separation ideas to understand the assumptions behind instrumental variables estimation Implement a simulation of data arising from an IV design Use simulation to understand properties of IV estimation Evaluate the validity of potential instruments in research settings Relate the IV approach to imperfect randomized trials Warm-up BREAKING: Randomized trial EPICOS will study #tenofovir for the prevention of #COVID19 in 4000 health care workers in Spain.Participants will receive tenofovir (an HIV drug) or hydroxychloroquine (a malaria drug) or both or placebo.Led by Julia del Amohttps://t.co/aLNf6Sytcm — Miguel Hernán (@_MiguelHernan) April 6, 2020 Question: For studying the effect of tenofovir on risk of developing COVID-19… What might a possible instrumental variables study design look like? What might a possible regression discontinuity design look like? What might a possible interrupted times series design look like? Discussion There are 4 core assumptions underlying an IV analysis: Relevance: The instrument is associated with the treatment Exclusion restriction: The instrument only affects the outcome through the treatment variable \\(A\\) (no direct effect of instrument on \\(Y\\)) Randomized assignment: The instrument and the outcome do not have common causes. The instrument is as if randomly assigned. Monotonicity: There are no “defiers”. When there are no defiers, the IV estimate ends up generalizing only to the compliers and is thus called a complier average causal effect (CACE) or a local average treatment effect LATE). Let \\(A^{IV}\\) be the potential outcome for the treatment variable depending on the value of the instrument \\(IV\\). Four compliance types or principal strata: Always-takers: Individuals who always take treatment, regardless of what their IV assigned. i.e., \\(A^{IV=0} = 1, A^{IV=1} = 1\\) Never-takers: Individuals who never take treatment, regardless of what their IV assigned. i.e., \\(A^{IV=0} = 0, A^{IV=1} = 0\\) Compliers: Individuals who will take treatment when assigned and not when not assigned. i.e., \\(A^{IV=0} = 0, A^{IV=1} = 1\\) Defiers: Individuals who will take treatment when assigned and not when not assigned. i.e., \\(A^{IV=0} = 1, A^{IV=1} = 0\\) The first three assumptions are instrumental conditions. The fourth assumption affects the population to which the IV estimate generalizes. For more detail about assumption 4, see Chapter 16 of our WHATIF book. Exercises A template Rmd is available here. IV assumptions Using d-separation ideas, explain why the first three assumptions are needed for instrumental variables estimation. Simulation planning We will be implementing a simulation study to understand statistical properties of IV estimation. Before we jump in, let’s plan: How will we set up a setting where we know the true value of the causal effect? (Think about simulating DAGs.) We will also be thinking of uncertainty quantification. That is, we will be looking at properties of confidence intervals for effect estimates. Simulation: setting up helper functions Functions are a great way to organize code, repeat tasks, and avoid errors by modularizing key chunks of a workflow. We will construct several functions to help with our simulation. First, complete the get_sim_data() function below. (There are several missing pieces.) This function simulates data according to the DAG shown in our video. Note that it takes 3 arguments. How are these arguments used within the function? If you need to simulate any exogenous variables (variables with no causes), simulate them with a 50% probability of equaling 1. library(dplyr) get_sim_data &lt;- function(n, p_A_IV0, p_A_IV1) { p_AfromIV &lt;- dplyr::case_when( IV==1 ~ p_A_IV1, IV==0 ~ p_A_IV0 ) p_AfromC &lt;- dplyr::case_when( C==1 ~ 0.5, C==0 ~ 0.2 ) p_A &lt;- p_AfromIV*p_AfromC A &lt;- rbinom(n, size = 1, prob = p_A) A_IVdo0 &lt;- rbinom(n, size = 1, prob = p_A_IVdo0) A_IVdo1 &lt;- rbinom(n, size = 1, prob = p_A_IVdo1) p_YfromA &lt;- dplyr::case_when( A==1 ~ 0.6, A==0 ~ 0.3 ) p_YfromC &lt;- dplyr::case_when( C==1 ~ 0.7, C==0 ~ 0.4 ) p_Y &lt;- p_YfromA*p_YfromC Y &lt;- rbinom(n, size = 1, prob = p_Y) Y_Ado0 &lt;- rbinom(n, size = 1, prob = p_Y_Ado0) Y_Ado1 &lt;- rbinom(n, size = 1, prob = p_Y_Ado1) sim_data &lt;- data.frame(IV, C, A, A_IVdo0, A_IVdo1, Y, Y_Ado0, Y_Ado1) sim_data } Next, complete the functions below to obtain the ACE (\\(P(Y^{a=1}=1)-P(Y^{a=0}=1)\\)) and the CACE. These functions take as input a data argument. We will be supplying the results of get_sim_data(), so you can assume that the sim_data variable names are present. Examples of using filter(): filter(A==1) filters down to the treated filter(A==1 | C==1) filters down to the rows where A = 1 OR C = 1 filter(A==1 &amp; C==1) filters down to the rows where A = 1 AND C = 1 get_ace &lt;- function(data) { } get_cace &lt;- function(data) { data_subs &lt;- data %&gt;% filter() } Next complete the get_iv_estimate() function below to implement the instrumental variables estimate of the causal effect of treatment: get_iv_estimate &lt;- function(data) { component1 &lt;- data %&gt;% group_by(IV) %&gt;% summarize(???) %&gt;% pull(???) %&gt;% diff() } The bootstrap_iv_estimate() function below performs 1000 bootstrapping iterations to get 1000 IV estimates. Add comments to document what each piece of code does. bootstrap_iv_estimate &lt;- function(data) { replicate(1000, { data_resampled &lt;- data %&gt;% sample_n(size = nrow(data), replace = TRUE) get_iv_estimate(data_resampled) }) } Running the simulation Now let’s actually run the simulation study! The for-loop below loops over different values of a parameter \\(p\\) for \\(p \\in \\{0.05,0.15,0.25,0.35,0.45\\}\\). Fill in the question marks below. When you run this code, it will take a little while. set.seed(394) for (p in seq(0.05,0.45,0.1)) { cat(&quot;p=&quot;, p, &quot;\\n&quot;) sim_data &lt;- get_sim_data(n = 1e5, p_A_IV0 = p, p_A_IV1 = 1-p) ace &lt;- ??? cace &lt;- ??? cat(&quot;ACE (whole pop):&quot;, ace, &quot;\\n&quot;) cat(&quot;CACE:&quot;, cace, &quot;\\n&quot;) iv_estim &lt;- get_iv_estimate(sim_data) boot_iv_results &lt;- bootstrap_iv_estimate(sim_data) boot_ci &lt;- quantile(boot_iv_results, probs = ???) # 95% CI cat(&quot;IV results:\\n&quot;) cat(&quot; Estimate:&quot;, iv_estim, &quot;\\n&quot;) cat(&quot; 95% bootstrap CI:&quot;, boot_ci, &quot;\\n&quot;) cat(&quot; CI width:&quot;, diff(boot_ci), &quot;\\n\\n&quot;) } Form some conclusions from the results: There is a notion of an instrument being “strong” or “weak”, which relates to how strongly the instrument influences/predicts the treatment \\(A\\). Does increasing \\(p\\) correspond to a stronger or weaker instrument? What do you notice about how well the IV estimate predicts the average causal effect? The complier average causal effect? What do you notice about how the confidence intervals change with changing instrument strength? Instruments in practice Some examples of instruments used in health services research are below. In light of the assumptions underlying IV estimation, do you think that these instruments could be valid? (Validity of an instrument refers to the assumptions being met.) What threats to validity do you foresee? Randomized encouragement designs: Physicians or patients are randomly assigned to receive encouragement (e.g., promotional materials) to use certain treatments. Preference-based instruments: individual physicians, hospitals, or care providers have varied and “random” preferences for one treatment over others. Distance to a specialty care provider: A special health service may only be offered at certain locations. Distance to such a specialty care provider might be random across patients and also impact whether or not a patient receives the special health service. This article provides more examples and is also a great review article about instrumental variables in general. It has great discussion about considerations for reporting results from IV analyses and also discusses a generalization of our IV estimator (also called the Wald estimator): two-stage least squares. If you are curious about any of these ideas, ask the instructor! IV estimation and RCTs The issue of noncompliance in randomized trials was raised in the video. Noncompliance occurs when an individual does not comply with the treatment that was randomly assigned. Compared to the usual causal diagram depicting a (perfect) randomized trial, how does a randomized trial with noncompliance compare? How could instrumental variables estimation be useful in this setting? In an IV analysis, to whom does the effect estimate generalize, and why might this be a concern in practice? "],
["its-and-rd-designs.html", "Topic 13 ITS and RD designs Learning Goals Exercises", " Topic 13 ITS and RD designs Learning Goals Propose appropriate models that can model data from interrupted time series, difference-in-difference, and regression discontinuity designs Interpret relevant coefficients in those models Analyze limitations of results in these settings Exercises Regression discontinuity models The Head Start program is a large program that was started in 1965 to give financial aid to the families of low income children ages 3 to 4. The government sent officials to help the 300 poorest counties draft grant applications for Head Start funding. On average, these applications did in fact increase Head Start funding in these poorest counties. Investigators looked at subsequent impacts on health outcomes (e.g., mortality rates) and educational outcomes (e.g., high school graduation rates). Part a Explain how poverty rates can be viewed as a kind of randomization to receiving Head Start funding or not. Part b The model below allows for estimation of the causal effect of Head Start funding on graduation rates: \\[ E[Y] = \\beta_0 + \\beta_1 A + \\beta_2 C \\] \\(C\\): poverty rates \\(A\\): 1 if received Head Start funding, 0 otherwise \\(Y\\): graduation rates What coefficient or combination of coefficients represents the causal effect? To whom does this causal effect generalize? Part c It turns out that results of regression discontinuity analyses are quite sensitive to the form of the model chosen. Draw a picture that shows a curved (e.g., quadratic) relationship between graduation rates and poverty rates with no causal effect of Head Start funding. What would be the results of fitting the model in part (b)? Part d Let’s zoom out and try to integrate contextual thinking with ideas from throughout our course. In the context of the Head Start study, what concerns might arise about study implementation or validity of results? What recommendations do you have for planning future studies of Head Start? Interrupted time series models Typically these models use autoregressive and/or moving average types of time series model to model trends over time. If you are curious about these methods, STAT 452 Correlated Data is the class for you! We won’t discuss time series models, but we can gain a lot of intuition for how these models are used in practice by examining linear regression models. Part a The general form of a linear regression model for an interrupted time series design is below: \\[ E[Y] = \\beta_0 + \\beta_1 T + \\beta_2 I + \\beta_3 TI + \\beta_4 A + \\beta_5 AT + \\beta_6 AI + \\beta_7 AIT \\] \\(Y\\): outcome/response variable \\(T\\): time \\(I\\): 1 if in the time period post-intervention, 0 for pre-intervention \\(A\\): 1 for treatment sites receiving the intervention, 0 for control sites Draw a figure showing the relationship between \\(Y\\) and \\(T\\) and showing how the slopes change over time and in between treatment vs. control sites. Label slopes, intercepts, and any changes or discontinuities with model coefficients. Part b When picking control sites, it is best to pick them such that they are as similar as possible to the treatment states. If this is done well, for which coefficients should we expect the confidence intervals to overlap zero? Part c Which coefficients represent the causal effect of the intervention, and how can we interpret them? Part d Policy researchers often use interrupted time series designs to understand the causal effect of a law or policy. What concerns might arise about results from an interrupted time series analysis? Part e In economics, a popular study design to estimate causal effects is called the difference-in-difference design. This design is a special case of the interrupted time series design with only one time point measured pre- and post-intervention for the treatment and control sites. Let our variables be as follows: \\(Y\\): outcome/response variable \\(T\\): 1 if in the time period post-intervention, 0 for pre-intervention \\(A\\): 1 for treatment sites receiving the intervention, 0 for control sites Write a regression model formula that can estimate the causal effect. As with the more general interrupted times series design, what are we assuming about the time trends of the outcome if the intervention had not been applied? "],
["homework-1.html", "Homework 1 Conceptual Exercises Simulation Exercises Portfolio", " Homework 1 Due Thursday, February 13 in class Conceptual Exercises These exercises can be turned in on paper in class. Exercise 1 We have looked at 3 DAG structures (chains, forks, and colliders) and claimed that they are the building blocks of all DAGs. Is this true? We will verify this premise in this exercise. Let’s start by considering all possible directed (not necessarily acyclic) graphs with 3 variables. (Three is the smallest number of nodes where any interesting structures can arise.) There are 27 of these graphs. Draw all of these graphs with the aid of the expand.grid() function in R. First, look up the documentation for this function. Second, run the command below to get a sense for how the function can help you. For this part, display the 27 graphs, and provide a brief explanation of how expand.grid() helped. expand.grid( X_to_Y = c(&quot;-&gt;&quot;, &quot;&lt;-&quot;, &quot;absent&quot;), Y_to_Z = c(&quot;-&gt;&quot;, &quot;&lt;-&quot;, &quot;absent&quot;), X_to_Z = c(&quot;-&gt;&quot;, &quot;&lt;-&quot;, &quot;absent&quot;) ) From your list of graphs, indicate which graphs can be eliminated because they are cyclic. From your list of graphs, indicate which graphs can be eliminated because they are useless causal models. In your list of graphs, some graphs are essentially the same. Group these together, and draw one representative graph from each group. Briefly form a conclusion based on these investigations. Exercise 2 PRIMER, Study question 2.3.1: Parts (c), (e), and (f). For parts (e) and (f), explain your thinking. Exercise 3 PRIMER, Study question 2.4.1: Parts (a), (b), and (c). For part (b), only d-separate pairs of non-adjacent measured variables. Simulation Exercises On Moodle, turn these exercises in by submitting both your Rmd file and the knitted HTML. For these exercises, ensure that your plots are well-labeled. Add the following to the ends of your plots to label your plots accurately: + labs(x = &quot;x axis label&quot;, y = &quot;y axis label&quot;, title = &quot;overall plot title&quot;) A refresher on facet_grid() that may be useful: To form panels across columns by variable Y: + facet_grid(~ Y) To form a grid of panels where rows correspond to X and columns to Y: + facet_grid(X ~ Y) To form a grid of panels where rows correspond to X and columns to Y and Z: + facet_grid(X ~ Y+Z) In case you’re curious about how to change the labeling of facet titles, see here. (Optional) You may or may not find it helpful/enjoyable to use the gridExtra package for arranging your plots. Example use case: library(gridExtra) p1 &lt;- ggplot(...) # Plot 1 p2 &lt;- ggplot(...) # Plot 2 p3 &lt;- ggplot(...) # Plot 3 p4 &lt;- ggplot(...) # Plot 4 # To arrange the 4 plots in a 2x2 grid: grid.arrange(p1, p2, p3, p4, ncol = 2, nrow = 2) If you look at your knitted HTML and think that your plots are too small, you can change the beginning of your code chunks to the following (playing with the numbers as needed): ```{r fig.width=12, fig.height=5} Exercise 4 Simulate data for a chain X -&gt; Y -&gt; Z where all variables are binary. Make plots to show the following properties: X and Z are marginally dependent X and Z are conditionally independent given Y Clearly explain how the plots show the dependence or independence. When you use dplyr::case_when() to generate probabilities that depend on another single variable, will any set of numbers work? Explain. Give an example of a set of numbers that would not generate the desired results. Exercise 5 Simulate data for a fork Y &lt;- X -&gt; Z where all variables are binary. Make plots to show the following properties: Y and Z are marginally dependent Y and Z are conditionally independent given X Exercise 6 Simulate data for a collider X -&gt; Z &lt;- Y where all variables are binary. Make plots to show the following properties: X and Y are marginally independent X and Y are conditionally dependent given Z When you use dplyr::case_when() to generate probabilities that depend on two variables, will any set of numbers work? Explain. Give an example of a set of numbers that would not generate the desired results. Exercise 7 Simulate data corresponding to the causal diagram in Figure 2.8 of PRIMER. Ignore the U variables (error terms) in the diagram, and treat all variables (T, Z, W, X, Y, and U) as binary. Make plots to show the following properties, and explain why they hold using the rules of d-separation: Z and Y are marginally dependent Z and Y are conditionally independent given T Z and Y are conditionally dependent given T and W Z and Y are conditionally independent given T, W, and X Portfolio Work on a draft post for required topic #1 (the birth weight paradox). "],
["homework-2.html", "Homework 2 In-class activities: Estimating causal effects In-class activities: IP weighting in practice", " Homework 2 Due Tuesday, February 25 at midnight on Moodle In-class activities: Estimating causal effects Turn in required parts of the class exercises from Topic 5: Estimating causal effects. These parts include: Simulation example: Tasks 1 and 2 On your own: Questions and Tasks 1 to 5 In-class activities: IP weighting in practice Turn in all class exercises from Topic 6: IP weighting in practice "],
["homework-3.html", "Homework 3", " Homework 3 Due Wednesday, April 8 at midnight (CST) on Moodle Turn in all exercises from our Mediation topic: Topic 10: Graphical Structure of Mediation Topic 11: Applied Mediation Analysis Portfolio: Write up a draft post for Post 2: Inverse probability weighting. See the Portfolio page for more details. Also revise your draft on the birth weight paradox (Post 1) given my earlier comments. Tag me in a comment if you are ready for a re-read. "],
["portfolio-1.html", "Portfolio Required Topics Iterative Progress Building a Personal Website", " Portfolio Your portfolio is meant to be a well-written collection of documents about course topics. The hope is that this collection is a useful reference for yourself years down the line, but potentially it will be helpful for others who may happen across your material. The Tweet below also motivates the portfolio well: Want to get better at #datascience?🎓 TEACH!📖 Write blog posts🗣️ Speak at meetups🎥 Record tutorials❓ Answer questions online🙋‍♀️ Tutor your friendsTeaching encourages you to clarify your thoughts, fill in gaps in your knowledge &amp; communicate clearly.Agree/disagree? — Kevin Markham (@justmarkham) February 17, 2020 Because of these goals, your portfolio will be a collection of blog posts. If you are keen on building your own website, you could publish these posts there as a part of building up an online presence. This is certainly not a requirement for class though. How you organize and structure your posts is up to you, but they need to address the topics below. Required Topics (This list will grow as the semester progresses.) Post 1 An analysis of the article The Birth Weight “Paradox” Uncovered? at the level of detail of our in-class discussion. The post does not have to answer the exact same discussion questions posed in class. The post should explain the paradox and what the authors did to investigate it. This should include a discussion of how they proceeded through the various causal diagrams and why some were eliminated. The post should explain how the authors resolved the paradox in the most realistic causal diagrams. The post should conclude with general lessons learned from this example. Post 2 Explain how we derived inverse probability weighting as a way to estimate average causal effects. Audience: You should write for an audience of your MSCS major classmates who only know about d-separation. In your post you should discuss an example of the tree diagrams we used to motivate the inverse probability weighting methodology. You do not need to discuss IP weighting for censoring. Iterative Progress You will work on these posts iteratively throughout the semester. The homework assignments will give deadlines for when a rough draft is due. Although writing rough drafts will be part of your homework duties, the drafts will not be part of your homework grade. You will only receive qualitative feedback from me on your drafts. This will be the process for writing and feedback for the semester: Create one (and only one) new Google Doc. This will be where all of your drafts go. Enable link sharing so that those with the link can edit. Submit the link on Moodle &gt; Homework &gt; Portfolio Google Doc Link. Start writing! In the days after draft due dates, I will be reading your drafts and giving feedback by a fixed time. You are welcome to revise based on my feedback as many times as you want. When you would like another round of feedback, add a comment to the relevant section and tag me in it to bring it to my attention. (Type a “+” in the comment box followed by my email address.) Building a Personal Website If you are curious about building your own website, read on! First check out the instructor’s website. If you like the layout and want to try making a website using R (the blogdown package), read her blog post about setting up her website. "]
]
